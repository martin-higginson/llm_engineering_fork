{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T05:24:40.690366Z",
     "start_time": "2025-07-02T05:24:39.442884Z"
    }
   },
   "source": [
    "# imports\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T05:24:45.876156Z",
     "start_time": "2025-07-02T05:24:45.872726Z"
    }
   },
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "OLLAMA_HEADERS = {\"Content-Type\": \"application/json\"}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T05:24:53.592049Z",
     "start_time": "2025-07-02T05:24:52.314389Z"
    }
   },
   "source": [
    "# set up environment\n",
    "!ollama pull llama3.2\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai = OpenAI()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠋ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠙ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠹ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠸ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠼ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠴ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠦ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ⠧ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest \u001B[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001B[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001B[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001B[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001B[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001B[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001B[K\n",
      "verifying sha256 digest \u001B[K\n",
      "writing manifest \u001B[K\n",
      "success \u001B[K\u001B[?25h\u001B[?2026l\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T05:24:56.976364Z",
     "start_time": "2025-07-02T05:24:56.965744Z"
    }
   },
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "def system_prompt():\n",
    "   return \"\"\"\n",
    "    You are a Python code tutor. You answer questions about the Python language and provide explanations. Give your responses in Markdown format.\"\n",
    "    \"\"\"\n",
    "def user_prompt():\n",
    "    return question\n",
    "\n",
    "def messages():\n",
    "    return [\n",
    "     {\"role\": \"system\", \"content\": system_prompt()},\n",
    "     {\"role\": \"user\", \"content\":user_prompt()}\n",
    "    ]\n",
    "def olama_payload():\n",
    "    return {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": messages(),\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "def display_markdown(md):\n",
    "    display(Markdown(md))\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T05:25:17.654998Z",
     "start_time": "2025-07-02T05:25:06.647256Z"
    }
   },
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt()},\n",
    "        {\"role\": \"user\", \"content\": user_prompt()}\n",
    "      ],\n",
    ")\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(result))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Certainly! Let's break down the code snippet you provided:\n\n```python\nyield from {book.get(\"author\") for book in books if book.get(\"author\")}\n```\n\n### Explanation\n\n1. **Context of `yield`**:\n   - The keyword `yield` is used in Python to make a function a generator. A generator is a type of iterable that generates values on the fly and is used to handle large datasets efficiently by not storing all values in memory at once.\n\n2. **`yield from`**:\n   - The `yield from` syntax is used to delegate part of a generator's operations to another generator. In this case, it will yield each item produced by the iterable that follows it.\n\n3. **Set Comprehension**:\n   - The part `{book.get(\"author\") for book in books if book.get(\"author\")}` is a **set comprehension**. \n   - It creates a set of authors from a collection of `books`. \n   - Here’s how it works:\n     - `for book in books`: Iterates over each `book` in the `books` collection (which is assumed to be a list or similar iterable).\n     - `book.get(\"author\")`: This retrieves the value associated with the key `\"author\"` from each `book` dictionary. If the key doesn't exist, it returns `None`.\n     - `if book.get(\"author\")`: This ensures that only books which have a non-`None` author are considered. If `get` returns `None` (i.e., if the author key does not exist), that book is filtered out and won't be included in the set.\n     - The result is a set of unique authors.\n\n### What the code does:\n\n- The entire expression creates a set of unique author names from a list of book dictionaries (`books`), excluding any entries that do not have an author specified (those with `None`).\n- Then, `yield from` takes each author from this set and yields them one by one when the generator function is called.\n\n### Example Usage\n\nLet’s say you have a list of books:\n\n```python\nbooks = [\n    {\"title\": \"Book A\", \"author\": \"Author 1\"},\n    {\"title\": \"Book B\", \"author\": \"\"},\n    {\"title\": \"Book C\", \"author\": \"Author 2\"},\n    {\"title\": \"Book D\"},\n    {\"title\": \"Book E\", \"author\": \"Author 1\"},\n]\n```\n\nUsing the `yield from` line you've provided would yield:\n- \"Author 1\"\n- \"Author 2\"\n\nSince \"Author 1\" appears in multiple books, it will only appear once in the set.\n\n### Summary\n\nTo summarize, the provided code snippet efficiently gathers unique authors from a list of books and yields them one at a time from a generator function, ignoring any books that do not have an author. This is useful in scenarios where you want to process or display authors without duplicating any."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "# response = requests.post(OLLAMA_API, json=olama_payload, headers=OLLAMA_HEADERS)\n",
    "# print(response.json()['message']['content'])\n",
    "\n",
    "print(messages())\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages())\n",
    "Markdown(response['message']['content'])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
